{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "링커리어_크롤러.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koominji/CrawlingLinkareer/blob/main/%EB%A7%81%EC%BB%A4%EB%A6%AC%EC%96%B4_%ED%81%AC%EB%A1%A4%EB%9F%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "036e634e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04b2decf-fc4b-48f9-b254-d41e762e65a0"
      },
      "source": [
        "!pip install selenium\n",
        "!pip install openpyxl"
      ],
      "id": "036e634e",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.1.0-py3-none-any.whl (958 kB)\n",
            "\u001b[K     |████████████████████████████████| 958 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.19.0-py3-none-any.whl (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 52.0 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure]~=1.26\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 40.3 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.2.0)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting cryptography>=1.3.4\n",
            "  Downloading cryptography-36.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 42.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure]~=1.26->selenium) (2021.10.8)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (2.21)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.7 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-36.0.0 h11-0.12.0 outcome-1.1.0 pyOpenSSL-21.0.0 selenium-4.1.0 sniffio-1.2.0 trio-0.19.0 trio-websocket-0.9.2 urllib3-1.26.7 wsproto-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a98ef2b0"
      },
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import Select\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import openpyxl"
      ],
      "id": "a98ef2b0",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e1c1399"
      },
      "source": [
        "wb = openpyxl.Workbook()\n",
        "sheet= wb.active\n",
        "sheet.append([\"index\",\"view\",\"title\",\"content\"])\n",
        "\n",
        "#크롬드라이버 경로설정\n",
        "driver_path = 'chromedriver.exe'\n",
        "driver = webdriver.Chrome(executable_path=driver_path)\n",
        "driver.implicitly_wait(3)\n",
        "#오픈할 브라우저 url\n",
        "driver.get(\"https://community.linkareer.com/board_Efog91\")\n",
        "driver.implicitly_wait(3)\n",
        "\n",
        "time.sleep(0.5)\n",
        "\n",
        "addrUrl = []\n",
        "page = 1\n",
        "# p = driver.page_source\n",
        "# soup = BeautifulSoup(p,\"html.parser\") #html로 파싱\n",
        "while page < 25: # 24페이지까지만 (수집기간 약 1년)\n",
        "    try:\n",
        "        time.sleep(1)\n",
        "        driver.get(f\"https://community.linkareer.com/board_Efog91?page={page}&field=title&word=&isNotice=false&isBest=false\")\n",
        "        # 링커리어 -> 취준생이야기방 게시판\n",
        "        page += 1\n",
        "        try:\n",
        "            if driver.find_element_by_class_name(\"MuiTableCell-root jss414 MuiTableCell-body\").text == '게시글이 없습니다.':\n",
        "                # 혹시 다음페이지에 내용이 없어서 '게시글이 없습니다'가 나오면 다음페이지로 넘어가는것을 멈춤\n",
        "                break\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        for i in range(1, 21, 1): # 한 페이지에 20개의 게시글 존재\n",
        "            addrUrl.append(driver.find_element_by_xpath(f'//*[@id=\"__next\"]/div/div[4]/div/div[2]/div/div/div[2]/table/tbody/tr[{i}]/td/div/div[1]/div/a').get_attribute(\"href\"))\n",
        "            # 각 게시글 URL을 addURL리스트에 추가\n",
        "        driver.switch_to.default_content()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "title_list = []\n",
        "content_list = []\n",
        "view_count=[]\n",
        "for url in addrUrl:\n",
        "    time.sleep(2)\n",
        "    driver.get(url) # 리스트에 저장된 각각의 게시글 URL을 엶\n",
        "\n",
        "    title_list.append(driver.find_element_by_css_selector('div:nth-child(1) h1').text) #게시글 제목을 title_list에 추가\n",
        "\n",
        "    pg = driver.page_source\n",
        "    soup = BeautifulSoup(pg, \"html.parser\")  # html로 파싱\n",
        "    articles = soup.find_all('a', {'class': 'jss451'})\n",
        "    each_content_page = driver.page_source\n",
        "    content_info_soup = BeautifulSoup(each_content_page, \"html.parser\")\n",
        "    contents = content_info_soup.find_all('p', {'class': 'block'})\n",
        "\n",
        "    result = \"\"\n",
        "    \n",
        "    for c in contents:\n",
        "        sentence = c.text\n",
        "        if sentence[:3] == \"로그인\":\n",
        "            print(\"\")  # '로그인'이라는 단어들어간 문장 버림(공통적으로 수집되는 불필요한 부분 제거)\n",
        "        else:\n",
        "            result += sentence\n",
        "    content_list.append(result) # 수집된 본문+댓글을 content_list에 추가\n",
        "\n",
        "    # 조회수 수집\n",
        "    view = content_info_soup.find_all('p', {'class': 'MuiTypography-root MuiTypography-body1 MuiTypography-colorTextSecondary'})\n",
        "    v = view[2].text\n",
        "    view_count.append(v[3:])\n",
        "\n",
        "\n",
        "#csv로 변환하기 위해 각 리스트를 데이터프레임에 담음\n",
        "df = pd.DataFrame({\"title\": title_list, \"content\": content_list, \"views\": view_count})\n",
        "df.to_csv('../output/linkareer_final.csv', encoding=\"utf-8-sig\")\n",
        "driver.quit()"
      ],
      "id": "0e1c1399",
      "execution_count": null,
      "outputs": []
    }
  ]
}